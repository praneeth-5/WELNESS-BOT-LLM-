{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1AD1QlHy5Qh9s7wmJX9JS0zi2p7uyw-jd","timestamp":1718945254956},{"file_id":"181QHaKInI5wNDPyfIj2sN9YWZBN-VSjW","timestamp":1718769848593}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"80b0bfa6011e406aac9d91a733d9eb90":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d37ca3d0d76a4651802ff49ac1dfa73c","IPY_MODEL_c8947054d0394ca797dcfe331a95f906","IPY_MODEL_a2e6a3995aac48aba3a5d81ac1eb63ee"],"layout":"IPY_MODEL_42be9ee02c8d4ba7b0035bab59029512"}},"d37ca3d0d76a4651802ff49ac1dfa73c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebab47bf3ba7491f9b3983efb528322c","placeholder":"​","style":"IPY_MODEL_a22ff3593dcb4d55b0b6da99733c9e5c","value":"config.json: 100%"}},"c8947054d0394ca797dcfe331a95f906":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5af8cd0e660945898042b7306bfcc29e","max":608,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b48fe8682e854857807b17af9eb20cfa","value":608}},"a2e6a3995aac48aba3a5d81ac1eb63ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_336611ae3b904cd39bab2a3ec3f39c1b","placeholder":"​","style":"IPY_MODEL_c698c9071b6b43cd8cfb8a4a90bd1a26","value":" 608/608 [00:00&lt;00:00, 18.8kB/s]"}},"42be9ee02c8d4ba7b0035bab59029512":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebab47bf3ba7491f9b3983efb528322c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a22ff3593dcb4d55b0b6da99733c9e5c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5af8cd0e660945898042b7306bfcc29e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b48fe8682e854857807b17af9eb20cfa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"336611ae3b904cd39bab2a3ec3f39c1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c698c9071b6b43cd8cfb8a4a90bd1a26":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c4f4345902c42458bc846a323863676":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_12c41f13fdbc422280afa0b9ccc6a491","IPY_MODEL_2ec7d6bb4a754219be1e9981b6308868","IPY_MODEL_13082dc3b58945ad9be245233a630d68"],"layout":"IPY_MODEL_3933404d829442c998b286ad5f4f8613"}},"12c41f13fdbc422280afa0b9ccc6a491":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be8cdff0c5e440d79dbc7ce88cbee80c","placeholder":"​","style":"IPY_MODEL_4f881d9cf8ff401d96c6e06200a27a4a","value":"model.safetensors: 100%"}},"2ec7d6bb4a754219be1e9981b6308868":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee392752f8a343419e8d0dca7784e4fd","max":2200119864,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ebaad6cb63ab43cca9ce125a1ba28c7b","value":2200119864}},"13082dc3b58945ad9be245233a630d68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_640902bdf6a64e8c88ac6bb43b00103d","placeholder":"​","style":"IPY_MODEL_8c86b17168f3470ab7a06e225274eabd","value":" 2.20G/2.20G [00:34&lt;00:00, 67.3MB/s]"}},"3933404d829442c998b286ad5f4f8613":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be8cdff0c5e440d79dbc7ce88cbee80c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f881d9cf8ff401d96c6e06200a27a4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee392752f8a343419e8d0dca7784e4fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebaad6cb63ab43cca9ce125a1ba28c7b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"640902bdf6a64e8c88ac6bb43b00103d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c86b17168f3470ab7a06e225274eabd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69cbbf6434934859a27b81e82db2cf36":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a0537533e8794bebbb587ab467c957ab","IPY_MODEL_70c671144b43422c8c47e6f5af3c14bb","IPY_MODEL_0b0430121d974af5bf3cf2dfb26a648c"],"layout":"IPY_MODEL_7492c591d9c24358bad69def7e64b15a"}},"a0537533e8794bebbb587ab467c957ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_640fab10d0b64fbc9996e669a5a3f475","placeholder":"​","style":"IPY_MODEL_a4ff1617e5cf4607929d01edc8cf7d5f","value":"generation_config.json: 100%"}},"70c671144b43422c8c47e6f5af3c14bb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5a601033b79428b80c15b8689645e94","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_69821389068f4a1d981ce03305423d5d","value":124}},"0b0430121d974af5bf3cf2dfb26a648c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6292fd681fa64387a891c8ec10299ef1","placeholder":"​","style":"IPY_MODEL_b12560081a504155a3802c6175a664cb","value":" 124/124 [00:00&lt;00:00, 9.11kB/s]"}},"7492c591d9c24358bad69def7e64b15a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"640fab10d0b64fbc9996e669a5a3f475":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4ff1617e5cf4607929d01edc8cf7d5f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5a601033b79428b80c15b8689645e94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69821389068f4a1d981ce03305423d5d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6292fd681fa64387a891c8ec10299ef1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b12560081a504155a3802c6175a664cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6611ec2dcf334d738719d931438e0be9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_32b84a2199e647998f4618a577cf7705","IPY_MODEL_8dd5a27044314bd6a76104c32b4731ba","IPY_MODEL_6bca0bbad4244600a1008906e07b7392"],"layout":"IPY_MODEL_539f831f77b94484859a533499e49a27"}},"32b84a2199e647998f4618a577cf7705":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcce41bc47b74a6eb4e6760ae1b5b3b8","placeholder":"​","style":"IPY_MODEL_729d97fcb3c840e9b4fded2b0efe2513","value":"tokenizer_config.json: 100%"}},"8dd5a27044314bd6a76104c32b4731ba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9111e600644a466987b64724fe5b7cd0","max":1289,"min":0,"orientation":"horizontal","style":"IPY_MODEL_61594c99f5f34c788091f8373dc6ecbf","value":1289}},"6bca0bbad4244600a1008906e07b7392":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c8eee5b324e44fcab0c450bf11d3ac1","placeholder":"​","style":"IPY_MODEL_579c79cdfa3a4ced9dcfc39977c41c6b","value":" 1.29k/1.29k [00:00&lt;00:00, 95.6kB/s]"}},"539f831f77b94484859a533499e49a27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcce41bc47b74a6eb4e6760ae1b5b3b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"729d97fcb3c840e9b4fded2b0efe2513":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9111e600644a466987b64724fe5b7cd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61594c99f5f34c788091f8373dc6ecbf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2c8eee5b324e44fcab0c450bf11d3ac1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"579c79cdfa3a4ced9dcfc39977c41c6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40be56f07c6d40b6af2cd5d969ee169f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b0519612d7ed40cb988ebc2db94bfe82","IPY_MODEL_590203098f9f4a3da5c272c8ce51c807","IPY_MODEL_301adb1bab004e3e925278ca14acc989"],"layout":"IPY_MODEL_36cddb637853440dbcf80de0f3aaa469"}},"b0519612d7ed40cb988ebc2db94bfe82":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5dfaba57897d485b8a95d2b5143f6f82","placeholder":"​","style":"IPY_MODEL_08ac6c5aadca43009062c495dfb7b9f4","value":"tokenizer.model: 100%"}},"590203098f9f4a3da5c272c8ce51c807":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_974069cf6b064b1ea444428da4d0a33b","max":499723,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d4773bef1d32405e99788fdc8f933555","value":499723}},"301adb1bab004e3e925278ca14acc989":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_872226c870674f0a81e637ff4acebef1","placeholder":"​","style":"IPY_MODEL_641b3af577db4ffdbe77f1bdf00323c7","value":" 500k/500k [00:00&lt;00:00, 35.2MB/s]"}},"36cddb637853440dbcf80de0f3aaa469":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dfaba57897d485b8a95d2b5143f6f82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08ac6c5aadca43009062c495dfb7b9f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"974069cf6b064b1ea444428da4d0a33b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4773bef1d32405e99788fdc8f933555":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"872226c870674f0a81e637ff4acebef1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"641b3af577db4ffdbe77f1bdf00323c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"860c1733362949069d9276484bf498d0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ed664dfbaf942dea220166e69b8b8ca","IPY_MODEL_827d9ab866944f4180ab6ec38c9dc6b0","IPY_MODEL_6ad3b12eb8b34b0b85f7d504b33a258b"],"layout":"IPY_MODEL_9dd1ac238c034e9b928487a6c9904ded"}},"4ed664dfbaf942dea220166e69b8b8ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab861461db53497eb44f5130a1549475","placeholder":"​","style":"IPY_MODEL_41566f50f7f340d6882649bd37dc6b58","value":"tokenizer.json: 100%"}},"827d9ab866944f4180ab6ec38c9dc6b0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c452c772995e468aa7278c994c439c94","max":1842767,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9082bb932b2741e68fceea3be378db82","value":1842767}},"6ad3b12eb8b34b0b85f7d504b33a258b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc71c89836e2455cac2a9534fce78309","placeholder":"​","style":"IPY_MODEL_93345a57b20049bf86a0b43c5694a4d2","value":" 1.84M/1.84M [00:00&lt;00:00, 6.47MB/s]"}},"9dd1ac238c034e9b928487a6c9904ded":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab861461db53497eb44f5130a1549475":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41566f50f7f340d6882649bd37dc6b58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c452c772995e468aa7278c994c439c94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9082bb932b2741e68fceea3be378db82":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc71c89836e2455cac2a9534fce78309":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93345a57b20049bf86a0b43c5694a4d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89283fd0bbad4c13b0da2077316f1a99":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4775f980dfe44d37a07a19d51932849a","IPY_MODEL_86ed455176294567898109655f08d4b2","IPY_MODEL_5dd8a620feed44edb28e121675e56f35"],"layout":"IPY_MODEL_bed3c39605a54912b4bd8c835b30fb71"}},"4775f980dfe44d37a07a19d51932849a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0ded9a8dbd744679951a11fb0fae6d9","placeholder":"​","style":"IPY_MODEL_5c349d896dde4da982895ad3575811b3","value":"special_tokens_map.json: 100%"}},"86ed455176294567898109655f08d4b2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6424c9e93622420e8501f296c5688e23","max":551,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39b36d6356fe4c419bf97c7b6bf7c41f","value":551}},"5dd8a620feed44edb28e121675e56f35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b155152d28b4919894d1e6790619216","placeholder":"​","style":"IPY_MODEL_8ce82ad9f04f475a94b0d0469948d587","value":" 551/551 [00:00&lt;00:00, 45.7kB/s]"}},"bed3c39605a54912b4bd8c835b30fb71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0ded9a8dbd744679951a11fb0fae6d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c349d896dde4da982895ad3575811b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6424c9e93622420e8501f296c5688e23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39b36d6356fe4c419bf97c7b6bf7c41f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b155152d28b4919894d1e6790619216":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ce82ad9f04f475a94b0d0469948d587":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07915ad9d4b04a529f23a4071326830b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0aaacc11255c466a8e9df87cb97b6e56","IPY_MODEL_079e26e98dd64df1bf09a25490130f22","IPY_MODEL_ea62e902e37f4448843d5a7ca3ed1458"],"layout":"IPY_MODEL_056b561390c84fc8877b10138255451e"}},"0aaacc11255c466a8e9df87cb97b6e56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8171d0d7109a455e973cb99d077bc832","placeholder":"​","style":"IPY_MODEL_419f500d5a65478291a95622c6029d33","value":"Generating train split: "}},"079e26e98dd64df1bf09a25490130f22":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_795755bcc0634ea78e038754f5edb39d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7232ebaf3b5e4d1ca96c98f28296403a","value":1}},"ea62e902e37f4448843d5a7ca3ed1458":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b84c51b9d5504cf199767e906878cba2","placeholder":"​","style":"IPY_MODEL_917c81ab430947c7b6d2d8e8e8a1ede4","value":" 21/0 [00:00&lt;00:00, 361.40 examples/s]"}},"056b561390c84fc8877b10138255451e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8171d0d7109a455e973cb99d077bc832":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"419f500d5a65478291a95622c6029d33":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"795755bcc0634ea78e038754f5edb39d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"7232ebaf3b5e4d1ca96c98f28296403a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b84c51b9d5504cf199767e906878cba2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"917c81ab430947c7b6d2d8e8e8a1ede4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"315f2c5232e94a31939f04a8512576ce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ce6f8b9ff7ff4e46badf9cdf6cb1d98c","IPY_MODEL_fa3853319fdc44609eacc37dce53e1d0","IPY_MODEL_1a3f4eab647c4576ad04a0b1b82d9c25"],"layout":"IPY_MODEL_d2ee34e4f611441d887b4df86faa3918"}},"ce6f8b9ff7ff4e46badf9cdf6cb1d98c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b3f63378d3b427eab07cd3c19255c59","placeholder":"​","style":"IPY_MODEL_f73d80c84b5746bbb7f1346e21751d7c","value":"Map: 100%"}},"fa3853319fdc44609eacc37dce53e1d0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_23fc0c20491c4d4bab4827665cd90c45","max":21,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d938e424912544409033a8d72b5dcf81","value":21}},"1a3f4eab647c4576ad04a0b1b82d9c25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cda1b11062c847838586129f7f77a790","placeholder":"​","style":"IPY_MODEL_4fe8c74730e9492e9a03b9c00c5e2caf","value":" 21/21 [00:00&lt;00:00, 891.93 examples/s]"}},"d2ee34e4f611441d887b4df86faa3918":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b3f63378d3b427eab07cd3c19255c59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f73d80c84b5746bbb7f1346e21751d7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23fc0c20491c4d4bab4827665cd90c45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d938e424912544409033a8d72b5dcf81":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cda1b11062c847838586129f7f77a790":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fe8c74730e9492e9a03b9c00c5e2caf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91468c115eb0445e9fb6bcf1c4c183d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_feb1fabb188c48acbf558f5abf47b9a4","IPY_MODEL_b95d711c833e43cc85a65bbb20c8c269","IPY_MODEL_f34d8502032a49d290d96448b74a7f7a"],"layout":"IPY_MODEL_6aa18c27bf6c47d597b10ac63a1c68f6"}},"feb1fabb188c48acbf558f5abf47b9a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_256e688e89524edeafda4cd573052275","placeholder":"​","style":"IPY_MODEL_cf624f5f45464a92b6c6df7b9cf71975","value":"Map: 100%"}},"b95d711c833e43cc85a65bbb20c8c269":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_73751dc5f66c49698e5b720ffb67b8fc","max":21,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c884cd013fa64bc99a8793f0cb33349d","value":21}},"f34d8502032a49d290d96448b74a7f7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78fe4be42a594feebf4cc8e1490ce5d1","placeholder":"​","style":"IPY_MODEL_08fbda1c17d94c728479362a43a78cc3","value":" 21/21 [00:00&lt;00:00, 499.99 examples/s]"}},"6aa18c27bf6c47d597b10ac63a1c68f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"256e688e89524edeafda4cd573052275":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf624f5f45464a92b6c6df7b9cf71975":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73751dc5f66c49698e5b720ffb67b8fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c884cd013fa64bc99a8793f0cb33349d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"78fe4be42a594feebf4cc8e1490ce5d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08fbda1c17d94c728479362a43a78cc3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Fine-tune and quantize LLM in Google Colab using Q-LoRA\n","\n"],"metadata":{"id":"OSHlAbqzDFDq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GLXwJqbjtPho","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f0a2bb63-7668-4b66-8950-0916146874c2","executionInfo":{"status":"ok","timestamp":1718773384623,"user_tz":-330,"elapsed":102457,"user":{"displayName":"NANDAKISHOR M","userId":"08970568302496220184"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate\n","  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting peft\n","  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bitsandbytes\n","  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Collecting trl\n","  Downloading trl-0.9.4-py3-none-any.whl (226 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Collecting datasets (from trl)\n","  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tyro>=0.5.11 (from trl)\n","  Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.1)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n","  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n","Collecting pyarrow>=15.0.0 (from datasets->trl)\n","  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets->trl)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (2.0.3)\n","Collecting requests (from transformers)\n","  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash (from datasets->trl)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets->trl)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.9.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (4.0.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.1)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n","Installing collected packages: xxhash, shtab, requests, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, tyro, nvidia-cusolver-cu12, datasets, bitsandbytes, accelerate, trl, peft\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed accelerate-0.31.0 bitsandbytes-0.43.1 datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 peft-0.11.1 pyarrow-16.1.0 requests-2.32.3 shtab-1.7.1 trl-0.9.4 tyro-0.8.4 xxhash-3.4.1\n"]}],"source":["!pip install  accelerate peft bitsandbytes transformers trl"]},{"cell_type":"code","source":["!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python\n","#pip install llama-cpp-python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-dGaBBNJwE8s","outputId":"4d6e11fe-70ae-43a8-8fd9-f1700b5632a2","executionInfo":{"status":"ok","timestamp":1718774521797,"user_tz":-330,"elapsed":1137177,"user":{"displayName":"NANDAKISHOR M","userId":"08970568302496220184"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-cpp-python\n","  Downloading llama_cpp_python-0.2.78.tar.gz (50.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n","Building wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.78-cp310-cp310-linux_x86_64.whl size=169129268 sha256=0bee53c4e3a9de128893035b2bbcf8ebcce1140d65fc80016d318ea7b9e40620\n","  Stored in directory: /root/.cache/pip/wheels/fd/c5/bd/3b1c20081bd71ce9d28b562573c97915c790bf1ef231879a61\n","Successfully built llama-cpp-python\n","Installing collected packages: diskcache, llama-cpp-python\n","Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.78\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer"],"metadata":{"id":"nAMzy_0FtaUZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## XLSX TO CSV"],"metadata":{"id":"-E3W51I-6c6P"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Read the XLSX file\n","xlsx_file = pd.read_excel('/content/training.xlsx')\n","\n","# Convert XLSX to CSV\n","xlsx_file.to_csv('output.csv', index=False)"],"metadata":{"id":"FsG6W9OToNf1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title prepare data\n","\n","input_prompt = \"\"\"Below is a Human Input, write appropriate Response based on the input.\n","\n","### Input:\n","{}\n","\n","### Response:\n","{}\"\"\"\n"],"metadata":{"id":"gQ889ixtala8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Detailed Explanation of Fine-Tuning Parameters:\n","\n","This script defines various parameters for fine-tuning a pre-trained model using Low-Rank Adapters (LoRA) and quantization techniques. Here's a breakdown of each section and its role in fine-tuning:\n","\n","**Model and Dataset:**\n","\n","* `model_name`: This specifies the pre-trained model you want to use for fine-tuning. Here, it's set to \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" from the Hugging Face hub.\n","* `new_model`: This defines the name you'll give to the fine-tuned model after training (here, \"tiny-llama-fine-tuned\").\n","\n","**LoRA Parameters:**\n","\n","* `lora_r`: This defines the dimension of the LoRA projection space. It controls the size of the additional parameters introduced for adaptation with LoRA.\n","* `lora_alpha`: This parameter controls the scaling applied to the LoRA weights during training.\n","* `lora_dropout`: This sets the dropout probability for the LoRA layers, helping to prevent overfitting.\n","\n","**BitsAndBytes Parameters (Quantization):**\n","\n","* `use_4bit`: This activates 4-bit precision for loading the base model, potentially reducing model size and inference speed.\n","* `bnb_4bit_compute_dtype`: This sets the computation data type for the 4-bit model (here, \"float16\").\n","* `bnb_4bit_quant_type`: This specifies the type of quantization used (here, \"nf4\").\n","* `use_nested_quant`: This enables nested quantization (double quantization), which might further reduce memory usage but could impact accuracy.\n","\n","**TrainingArguments Parameters:**\n","\n","* `output_dir`: This defines the directory where the model's predictions and checkpoints are saved during training (\"./results\" here).\n","* `num_train_epochs`: This sets the number of training epochs (iterations over the entire dataset). Here, it's set to 50.\n","* `fp16`, `bf16`: These enable mixed-precision training using 16-bit floating-point (fp16) or bfloat16 data types, potentially accelerating training on compatible hardware (set to False here).\n","* `per_device_train_batch_size`: This defines the number of training examples processed per GPU during each training step (set to 1 here). Similarly, `per_device_eval_batch_size` defines the batch size for evaluation.\n","* `gradient_accumulation_steps`: This accumulates gradients for multiple training steps before updating the model weights, potentially improving memory efficiency (set to 1 here).\n","* `gradient_checkpointing`: Enables gradient checkpointing, which saves memory by only storing a subset of activations during backpropagation (enabled here).\n","* `max_grad_norm`: This sets the maximum gradient norm for gradient clipping, preventing exploding gradients (set to 0.3 here).\n","* `learning_rate`: This defines the initial learning rate for the optimizer (AdamW here, set to 2e-4).\n","* `weight_decay`: This applies weight decay (L2 regularization) to all layers except bias and LayerNorm weights, helping to prevent overfitting (set to 0.001 here).\n","* `optim`: This specifies the optimizer used for training. Here, it's set to \"paged_adamw_32bit\".\n","* `lr_scheduler_type`: This defines the learning rate schedule. Here, \"cosine\" is used, which gradually reduces the learning rate over training.\n","* `max_steps`: This sets the total number of training steps (overrides `num_train_epochs`). Here, it's set to -1, meaning all epochs will be used.\n","* `warmup_ratio`: This defines the portion of training steps for a linear warmup of the learning rate (set to 0.03 here).\n","* `group_by_length`: This groups sequences of similar lengths into batches, improving memory efficiency and training speed (enabled here).\n","* `save_steps`: This sets the number of training steps between saving model checkpoints (set to 0 here, meaning no intermediate saves).\n","* `logging_steps`: This defines the number of training steps between logging training information (set to 25 here).\n","\n","**SFT Parameters:**\n","\n","* `max_seq_length`: This sets the maximum sequence length for training and inference (can be left as None).\n","* `packing`: This enables packing multiple short examples into a single input sequence to improve efficiency (disabled here).\n","* `device_map`: This defines which GPUs to use for training. Here, it maps all training to GPU 0 (\"\": 0).\n","\n","These parameters allow you to fine-tune the pre-trained model for a"],"metadata":{"id":"EvM7SaTx9Waf"}},{"cell_type":"code","source":["# The model that you want to train from the Hugging Face hub\n","model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","#model_mame = \"/content/final_weights_new\"\n","\n","# The instruction dataset to use\n","#dataset_name = \"mlabonne/guanaco-llama2-1k\"\n","\n","# Fine-tuned model name\n","new_model = \"tiny-llama-fine-tuned\"\n","\n","################################################################################\n","# QLoRA parameters\n","################################################################################\n","\n","# LoRA attention dimension\n","lora_r = 64\n","\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","\n","# Number of training epochs\n","num_train_epochs = 20\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","\n","# Batch size per GPU for training\n","per_device_train_batch_size = 1\n","\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 1\n","\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4 #0.0002 2x10-4\n","\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","\n","# Learning rate schedule\n","lr_scheduler_type = \"cosine\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","\n","# Save checkpoint every X updates steps\n","save_steps = 0\n","\n","# Log every X updates steps\n","logging_steps = 25\n","\n","################################################################################\n","# SFT parameters\n","################################################################################\n","\n","# Maximum sequence length to use\n","max_seq_length = None\n","\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0} # \"auto\""],"metadata":{"id":"ib_We3NLtj2E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Fine-tuning\n","Parameter-efficient fine-tuning (PEFT) is a technique used to adapt large pre-trained language models (LLMs) to new tasks while significantly reducing the number of parameters that need to be trained. Here's a breakdown of the key points:\n","\n","**Challenge of Fine-Tuning LLMs:**\n","\n","* LLMs are massive, with billions of parameters.\n","* Fine-tuning them on new tasks often requires training all these parameters, which can be:\n","    * Computationally expensive (takes a long time and requires powerful hardware).\n","    * Prone to overfitting (the model memorizes the training data instead of learning generalizable patterns).\n","\n","**PEFT Approach:**\n","\n","PEFT addresses these challenges by focusing on training only a small subset of the model's parameters while keeping the rest frozen. This allows for:\n","\n","* **Faster Training:** Less parameters to train means faster training times.\n","* **Reduced Memory Usage:** Smaller models require less memory on devices.\n","* **Improved Generalizability:** By not retraining everything, PEFT can help prevent overfitting and improve the model's ability to adapt to unseen data.\n","\n","**How PEFT Works:**\n","\n","There are several approaches to PEFT\n","\n","* **Low-Rank Adapters (LoRA):** Introducing a small set of additional parameters that act as \"adapters\" on top of the pre-trained model. These adapters allow the model to adapt to the new task without significantly changing the core parameters.\n","\n","**Benefits of PEFT:**\n","\n","* Enables fine-tuning LLMs on resource-constrained devices (e.g., mobile phones).\n","* Reduces training costs associated with large models.\n","* Can potentially improve the generalizability of the fine-tuned model.\n","\n","**Overall, PEFT is a valuable technique for making LLMs more accessible and adaptable to a wider range of tasks while keeping computational efficiency in mind.**\n","\n","Here's a breakdown of why 4-bit quantization is used and what happens to the vectors:\n","\n","**Why Use 4-Bit Quantization?**\n","\n","The code utilizes 4-bit quantization likely for two main reasons:\n","\n","1. **Reduced Model Size and Memory Usage:** Compared to using 32-bit floating-point numbers (FP32) for representing model weights and activations, 4-bit quantization (4 bits per number) significantly reduces the model size. This can be crucial for deploying the model on devices with limited memory, such as mobile phones or embedded systems.\n","\n","2. **Potentially Faster Inference:** While not guaranteed, using lower precision formats like 4-bit can sometimes lead to faster inference speeds on hardware that supports such operations efficiently. This can be beneficial for real-time applications where quick response times are important.\n","\n","**Is it Quantization-Aware Fine-Tuning?**\n","\n","The code snippet doesn't explicitly show if it's using quantization-aware fine-tuning. However, there are clues suggesting it might be:\n","\n","* **`BitsAndBytesConfig`:** This configuration likely controls the quantization settings.\n","* **Target Modules for LoRA:** Fine-tuning only specific modules (like those listed for LoRA) is a common approach when using quantization-aware fine-tuning. This allows for a balance between efficiency gains from quantization and maintaining accuracy.\n","\n","**What Happens to the Vectors During Quantization?**\n","\n","During 4-bit quantization, the original model's weights and activations (represented in FP32) are converted to 4-bit integers. This conversion process involves:\n","\n","1. **Scaling and Clipping:** The FP32 values are first scaled to a specific range suitable for representing with 4 bits. This might involve considering the minimum and maximum values of the original data.\n","2. **Rounding or Quantization:**  A specific strategy is used to convert the scaled values into 4-bit integers. This could involve rounding or other quantization techniques.\n","\n","**Impact on Accuracy:**\n","\n","Quantization, especially aggressive quantization like 4-bit, can introduce some loss of accuracy compared to the original FP32 model. However, the goal is to find a balance between reduced model size/inference speed and acceptable accuracy for the specific task.\n","\n","**Additional Notes:**\n","\n","* The code snippet mentions `bnb_4bit_quant_type` which likely specifies the exact quantization method used (e.g., linear quantization).\n","* The `compute_dtype` (e.g., bfloat16) might be related to the computations performed during training/inference with potentially lower precision formats for further efficiency gains.\n","\n","4-bit quantization aims to reduce model size and potentially speed up inference while considering the trade-off with accuracy."],"metadata":{"id":"67jRa9r788dZ"}},{"cell_type":"code","source":["# Load dataset (you can process it here)\n","#dataset = load_dataset(dataset_name, split=\"train\")\n","\n","# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","EOS_TOKEN = tokenizer.eos_token\n","def formatting_prompts_func(examples):\n","    inputs       = examples[\"Questions\"]\n","    outputs      = examples[\"Answers\"]\n","    texts = []\n","    for input, output in zip(inputs, outputs):\n","        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n","        text = input_prompt.format(input, output) + EOS_TOKEN\n","        texts.append(text)\n","    return { \"text\" : texts, }\n","pass\n","'''\n","def formatting_prompts_func(examples):\n","    inputs       = examples[\"instruction\"]\n","    outputs      = examples[\"output\"]\n","    texts = []\n","    for input, output in zip(inputs, outputs):\n","        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n","        text = input_prompt.format(input, output) + EOS_TOKEN\n","        texts.append(text)\n","    return { \"text\" : texts, }\n","pass'''\n","\n","from datasets import load_dataset\n","dataset = load_dataset('csv', data_files='output.csv',split=\"train\")\n","#dataset = load_dataset(\"nmdr/Mini-Physics-Instruct-1k\", split = \"train\")\n","dataset = dataset.map(formatting_prompts_func, batched = True,)\n","# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","      target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",")\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\",\n","\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n","\n",")\n","\n","# Train model\n","trainer.train()\n","\n","# Save trained model\n","trainer.model.save_pretrained(new_model)"],"metadata":{"id":"OJXpOgBFuSrc","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["80b0bfa6011e406aac9d91a733d9eb90","d37ca3d0d76a4651802ff49ac1dfa73c","c8947054d0394ca797dcfe331a95f906","a2e6a3995aac48aba3a5d81ac1eb63ee","42be9ee02c8d4ba7b0035bab59029512","ebab47bf3ba7491f9b3983efb528322c","a22ff3593dcb4d55b0b6da99733c9e5c","5af8cd0e660945898042b7306bfcc29e","b48fe8682e854857807b17af9eb20cfa","336611ae3b904cd39bab2a3ec3f39c1b","c698c9071b6b43cd8cfb8a4a90bd1a26","0c4f4345902c42458bc846a323863676","12c41f13fdbc422280afa0b9ccc6a491","2ec7d6bb4a754219be1e9981b6308868","13082dc3b58945ad9be245233a630d68","3933404d829442c998b286ad5f4f8613","be8cdff0c5e440d79dbc7ce88cbee80c","4f881d9cf8ff401d96c6e06200a27a4a","ee392752f8a343419e8d0dca7784e4fd","ebaad6cb63ab43cca9ce125a1ba28c7b","640902bdf6a64e8c88ac6bb43b00103d","8c86b17168f3470ab7a06e225274eabd","69cbbf6434934859a27b81e82db2cf36","a0537533e8794bebbb587ab467c957ab","70c671144b43422c8c47e6f5af3c14bb","0b0430121d974af5bf3cf2dfb26a648c","7492c591d9c24358bad69def7e64b15a","640fab10d0b64fbc9996e669a5a3f475","a4ff1617e5cf4607929d01edc8cf7d5f","b5a601033b79428b80c15b8689645e94","69821389068f4a1d981ce03305423d5d","6292fd681fa64387a891c8ec10299ef1","b12560081a504155a3802c6175a664cb","6611ec2dcf334d738719d931438e0be9","32b84a2199e647998f4618a577cf7705","8dd5a27044314bd6a76104c32b4731ba","6bca0bbad4244600a1008906e07b7392","539f831f77b94484859a533499e49a27","fcce41bc47b74a6eb4e6760ae1b5b3b8","729d97fcb3c840e9b4fded2b0efe2513","9111e600644a466987b64724fe5b7cd0","61594c99f5f34c788091f8373dc6ecbf","2c8eee5b324e44fcab0c450bf11d3ac1","579c79cdfa3a4ced9dcfc39977c41c6b","40be56f07c6d40b6af2cd5d969ee169f","b0519612d7ed40cb988ebc2db94bfe82","590203098f9f4a3da5c272c8ce51c807","301adb1bab004e3e925278ca14acc989","36cddb637853440dbcf80de0f3aaa469","5dfaba57897d485b8a95d2b5143f6f82","08ac6c5aadca43009062c495dfb7b9f4","974069cf6b064b1ea444428da4d0a33b","d4773bef1d32405e99788fdc8f933555","872226c870674f0a81e637ff4acebef1","641b3af577db4ffdbe77f1bdf00323c7","860c1733362949069d9276484bf498d0","4ed664dfbaf942dea220166e69b8b8ca","827d9ab866944f4180ab6ec38c9dc6b0","6ad3b12eb8b34b0b85f7d504b33a258b","9dd1ac238c034e9b928487a6c9904ded","ab861461db53497eb44f5130a1549475","41566f50f7f340d6882649bd37dc6b58","c452c772995e468aa7278c994c439c94","9082bb932b2741e68fceea3be378db82","dc71c89836e2455cac2a9534fce78309","93345a57b20049bf86a0b43c5694a4d2","89283fd0bbad4c13b0da2077316f1a99","4775f980dfe44d37a07a19d51932849a","86ed455176294567898109655f08d4b2","5dd8a620feed44edb28e121675e56f35","bed3c39605a54912b4bd8c835b30fb71","f0ded9a8dbd744679951a11fb0fae6d9","5c349d896dde4da982895ad3575811b3","6424c9e93622420e8501f296c5688e23","39b36d6356fe4c419bf97c7b6bf7c41f","7b155152d28b4919894d1e6790619216","8ce82ad9f04f475a94b0d0469948d587","07915ad9d4b04a529f23a4071326830b","0aaacc11255c466a8e9df87cb97b6e56","079e26e98dd64df1bf09a25490130f22","ea62e902e37f4448843d5a7ca3ed1458","056b561390c84fc8877b10138255451e","8171d0d7109a455e973cb99d077bc832","419f500d5a65478291a95622c6029d33","795755bcc0634ea78e038754f5edb39d","7232ebaf3b5e4d1ca96c98f28296403a","b84c51b9d5504cf199767e906878cba2","917c81ab430947c7b6d2d8e8e8a1ede4","315f2c5232e94a31939f04a8512576ce","ce6f8b9ff7ff4e46badf9cdf6cb1d98c","fa3853319fdc44609eacc37dce53e1d0","1a3f4eab647c4576ad04a0b1b82d9c25","d2ee34e4f611441d887b4df86faa3918","0b3f63378d3b427eab07cd3c19255c59","f73d80c84b5746bbb7f1346e21751d7c","23fc0c20491c4d4bab4827665cd90c45","d938e424912544409033a8d72b5dcf81","cda1b11062c847838586129f7f77a790","4fe8c74730e9492e9a03b9c00c5e2caf","91468c115eb0445e9fb6bcf1c4c183d5","feb1fabb188c48acbf558f5abf47b9a4","b95d711c833e43cc85a65bbb20c8c269","f34d8502032a49d290d96448b74a7f7a","6aa18c27bf6c47d597b10ac63a1c68f6","256e688e89524edeafda4cd573052275","cf624f5f45464a92b6c6df7b9cf71975","73751dc5f66c49698e5b720ffb67b8fc","c884cd013fa64bc99a8793f0cb33349d","78fe4be42a594feebf4cc8e1490ce5d1","08fbda1c17d94c728479362a43a78cc3"]},"outputId":"0ce8d373-ee5e-4ca5-e771-a8024aa8cef1","executionInfo":{"status":"ok","timestamp":1718775051779,"user_tz":-330,"elapsed":189156,"user":{"displayName":"NANDAKISHOR M","userId":"08970568302496220184"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80b0bfa6011e406aac9d91a733d9eb90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c4f4345902c42458bc846a323863676"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69cbbf6434934859a27b81e82db2cf36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6611ec2dcf334d738719d931438e0be9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40be56f07c6d40b6af2cd5d969ee169f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"860c1733362949069d9276484bf498d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89283fd0bbad4c13b0da2077316f1a99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07915ad9d4b04a529f23a4071326830b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/21 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"315f2c5232e94a31939f04a8512576ce"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:278: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/21 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91468c115eb0445e9fb6bcf1c4c183d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='420' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [420/420 02:19, Epoch 20/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>2.177400</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.976200</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.444700</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.204700</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.143000</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.095100</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.075600</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.059600</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>0.054000</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.051600</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>0.049200</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.046400</td>\n","    </tr>\n","    <tr>\n","      <td>325</td>\n","      <td>0.047000</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.044700</td>\n","    </tr>\n","    <tr>\n","      <td>375</td>\n","      <td>0.044300</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.043600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["## Self-Attention with Query, Key, Value\n","\n","Self-attention is a powerful mechanism in transformers that allows the model to focus on relevant parts of the input sequence when processing information. It works with three key components: query, key, and value.\n","\n","**Analogy:** Imagine you're at a party and want to find someone specific (the answer). You (the model) ask everyone at the party a question (the query) to identify potential matches. This question could be \"Are you interested in X?\". Everyone responds with a short description of themselves (the key). You then compare these descriptions to what you're looking for (compare query and key). Finally, you talk to the people whose descriptions seem most relevant (high comparison score) and get more information from them (the value).\n","\n","**Formally:**\n","\n","* **Query (Q):** A vector representing the current focus of attention. It's like your question at the party.\n","* **Key (K):** A vector representing each element in the input sequence. It's like the short description of each person at the party.\n","* **Value (V):** A vector containing the actual information associated with each element in the sequence. It's like the detailed information you get from the relevant people.\n","\n","The model calculates a score for each element in the sequence based on how well its \"key\" matches the \"query.\" Higher scores indicate a better match. Finally, the model uses these scores to weight the \"values\" from each element, creating a new representation that focuses on the most relevant parts of the sequence.\n","\n","**Example:**\n","\n","Consider the sentence \"The cat sat on the mat.\"\n","\n","* **Query:** The query vector could represent the word we're currently focusing on, say \"sat.\"\n","* **Key:** Each word in the sentence would have a key vector. For example, the key vector for \"cat\" might capture its semantic meaning (e.g., furry animal).\n","* **Value:** The value vector for each word would contain its embedding (numerical representation).\n","\n","The model would compare the query vector for \"sat\" with the key vectors of all words. The key vector for \"cat\" might have a higher score than others because \"sat\" often describes actions involving objects that can be sat upon. The model would then use this score to weight the value vector of \"cat,\" giving it more influence in the final representation.\n","\n","## Gate, Up-proj, Down-proj, and O\n","\n","These terms refer to specific parts within a transformer block that process information:\n","\n","**o_proj (Output projection)**:This linear layer is part of the self-attention mechanism. It projects the attention weights (scores for each element) back to the embedding dimension. This allows the model to combine the information from the relevant parts of the sequence into a single representation. This part remains the same as in the regular self-attention mechanism. It's not directly involved in the adaptation process with LoRA.\n","\n","\n","* **Gate:** This is a linear layer within the MLP (multi-layer perceptron) sub-block of a transformer. It takes the hidden state (current representation of the sequence) and projects it to a higher dimension. This creates a more complex representation before applying a non-linear activation function (like ReLU).\n","\n"," `gate_proj`, `up_proj`, and `down_proj` are all part of a transformer block, specifically within the **MLP sub-block**. They perform linear projections on the hidden state, which represents the current understanding of the sequence at that point in processing.\n","\n","Here's a breakdown of their roles and what they project to:\n","\n","* **gate_proj:** This linear layer projects the hidden state (current representation) to a **higher dimension**. This creates a more complex representation by allowing the model to capture a wider range of interactions between elements in the sequence.\n","\n","* **up_proj:** Following the `gate_proj`, this layer further projects the high-dimensional representation to an **even higher dimension**. This allows the model to explore even more intricate relationships within the sequence data.\n","\n","* **down_proj:** Finally, this layer projects the high-dimensional representation obtained from `up_proj` back to the **original embedding dimension**. This essentially compresses the information while still retaining the important details captured in the higher dimensional space.\n","\n","**Overall Flow:**\n","\n","1. The hidden state, representing the current understanding of the sequence, is fed into `gate_proj`.\n","2. `gate_proj` projects it to a higher dimension, creating a more complex representation.\n","3. `up_proj` takes this high-dimensional representation and projects it to an even higher dimension, allowing for exploration of intricate relationships.\n","4. Finally, `down_proj` projects the information back to the original embedding dimension, resulting in a compressed but informative representation.\n","\n","**Where it's Used:**\n","\n","This compressed representation is then fed into the final step of the transformer block, where it's combined with the residual connection (original hidden state) and a layer normalization step. This final output becomes the new hidden state for the next transformer block in the sequence, allowing the model to build a deeper understanding as it processes the entire sequence.\n","\n","**In Summary:**\n","\n","* `gate_proj`, `up_proj`, and `down_proj` are within the **MLP sub-block** of a transformer block.\n","* They project the hidden state to explore complex relationships in the sequence data.\n","* `gate_proj` and `up_proj` project to higher dimensions for more intricate analysis.\n","* `down_proj` projects back to the original dimension for a compressed but informative representation.\n","* This final representation is used to update the hidden state for the next transformer block."],"metadata":{"id":"F9OTa11S8XCb"}},{"cell_type":"code","source":["##Inference\n","inputs = tokenizer(\n","[\n","    input_prompt.format(\n","        \"who developed Nadi?\", # input\n","        \"\",   # leave blank as response generated by AI\n","\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","generated_text = tokenizer.batch_decode(outputs)[0]\n","first_response = generated_text.split('### Response:')[1].strip()\n","output = first_response.split('###')[0].strip()\n","print(\"the response is: \",output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LELHVNcqvX4T","outputId":"b12561d2-b230-46eb-e929-f23d4306289d","executionInfo":{"status":"ok","timestamp":1718776183407,"user_tz":-330,"elapsed":9570,"user":{"displayName":"NANDAKISHOR M","userId":"08970568302496220184"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["the response is:  Nadi was developed by Convai Innovations from Kerala, India. It is a offline Large Language Model (LLM) based AI PC application designed for AI education, eliminating the need for a real-time internet connection. Nadi's unique strength lies in its offline\n"]}]},{"cell_type":"code","source":["# Reload model in FP16 and merge it with LoRA weights w = w+del(w)\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=device_map,\n",")\n","model = PeftModel.from_pretrained(base_model, new_model)\n","model = model.merge_and_unload() #W=w+del(w)\n","\n","# Reload tokenizer to save it\n","#tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","#tokenizer.pad_token = tokenizer.eos_token\n","#tokenizer.padding_side = \"right\""],"metadata":{"id":"QQn30cRtAZ-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_dir = \"final_weights_new\"\n","model.save_pretrained(output_dir)"],"metadata":{"id":"rFH8fls61Ns5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Reload tokenizer to save it\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","tokenizer.save_pretrained(output_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2pES7_S11eTw","outputId":"a3f04056-27e3-423b-edfe-daca32fd13f5","executionInfo":{"status":"ok","timestamp":1718776921661,"user_tz":-330,"elapsed":1163,"user":{"displayName":"NANDAKISHOR M","userId":"08970568302496220184"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('final_weights_new/tokenizer_config.json',\n"," 'final_weights_new/special_tokens_map.json',\n"," 'final_weights_new/tokenizer.model',\n"," 'final_weights_new/added_tokens.json',\n"," 'final_weights_new/tokenizer.json')"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["# Huggingface inference of saved model"],"metadata":{"id":"HB6aXWY_M_uB"}},{"cell_type":"code","source":["# Run text generation pipeline with our next model\n","# Load model directly\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"/content/final_weights_new\")\n","model = AutoModelForCausalLM.from_pretrained(\"/content/final_weights_new\")\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=64)\n","prompt=input_prompt.format(\n","        \"who is Nandakishor?\", # input\n","        \"\", # leave blank as response generated by AI\n","\n","    )\n","result = pipe(prompt)\n","generated_text  = result[0]['generated_text']\n","\n","first_response = generated_text.split('### Response:')[1].strip()\n","first_response = first_response.split(\"\\n\")[0]\n","\n","print(first_response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y0widjE9-i9j","outputId":"e8d187e5-2b0d-4b67-9330-4207c582826b","executionInfo":{"status":"ok","timestamp":1718776969317,"user_tz":-330,"elapsed":22102,"user":{"displayName":"NANDAKISHOR M","userId":"08970568302496220184"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Nandakishor is an offline Large Language Model (LLM) based AI model designed for AI education, elimin\n"]}]},{"cell_type":"markdown","source":["#Quantization\n","## Key Concepts:\n","**GGUF (Giant GPT Unified Format)**: A model format designed for efficient storage and quantization of large Transformer-based language models like Llama.\n","Llama.cpp: A C++ library for working with GGUF models, including quantization tools.\n","\n","**LoRA (Low-Rank Adaptation)**: A technique for model efficiency and fine-tuning that involves adding adapter layers.\n","\n","**Quantization**: Converting floating-point model weights to lower-precision integers for reduced model size and faster inference.\n","\n","## Quantization Methods:\n","1. **Format Breakdown:**\n","Q#K[S/M/L]:#: Number of bits used (e.g., Q4 = 4 bits).\n","K: Represents low-rank matrix factorization for efficient storage.\n","[S/M/L]: Level of low-rank approximation:S: Small (moderate compression, high precision).\n","M: Medium (balance between compression and precision).\n","L: Large (aggressive compression, lower precision).\n","2. **Conversion Step:**\n","Imagine model weights residing in an apartment complex (FP16 format).\n","Conversion acts like a renovation:Rearrangement: Apartments are grouped and reorganized for efficient processing by quantization tools.\n","Pre-processing: Each apartment gets a thorough cleaning and preparation for the quantization \"paint job.\"\n","No actual quantization happens here; it's all about getting ready for the big transformation.\n","3. **Quantization Step:**\n","Now, the exciting transformation begins!\n","General Process:Calibration: Like measuring wall sizes before applying paint, optimal scaling factors are determined for each weight tensor.\n","Quantization: Weights are meticulously scaled and mapped to specific integer values within a limited range, like assigning each shade a specific paint color.\n","Matrix Factorization (K methods):Think of apartments being replaced with smaller studios (low-rank matrices) for some weights. This saves space and processing power.\n","Not all apartments get shrunk; only those deemed suitable for efficient compression.\n","Fine-tuning: After the renovation, some adjustments are needed. The model is fine-tuned, often using PEFT, to adapt to the quantization-induced \"color shifts\" and maintain accuracy.\n","Merged LoRA Weights:\n","Imagine LoRA adapters as extensions added to the apartment complex. They hold task-specific knowledge.\n","During quantization, these extensions go through the same process as the main building:Rearrangement for efficient processing.\n","Pre-processing for compatibility with quantization.\n","Calibration, scaling, and mapping to specific integer values (colors).\n","Selective matrix factorization for eligible weight tensors.\n","By treating LoRA weights equally, consistency and efficiency are maintained across the entire model after quantization.\n","Choosing the Right Method:\n","It's like picking the perfect renovation plan:Desired Size Reduction: How much do you want to shrink the apartment complex (model)?\n","Accuracy Trade-off: How much \"color change\" can you tolerate?\n","Hardware Compatibility: Will your neighbors (hardware) appreciate the new layout and materials?\n","Fine-tuning Resources: Do you have the tools and time to adjust to the changes?\n","Example: **Q4_K_M Explained:\n","This is like a moderate renovation:Walls get painted with specific \"4-color\" palettes (4-bit quantization).\n","Some rooms are converted into efficient studios (low-rank matrices) for better space utilization.\n","The balance between space saving and accuracy is carefully considered** (medium level of compression).\n","Additional Note:\n","Q8_0 is like keeping some rooms intact (without full quantization). They remain spacious (FP16), offering some size reduction but less efficiency compared to full renovations.\n"],"metadata":{"id":"WuzmlHPn109h"}},{"cell_type":"code","source":["!git clone https://github.com/ggerganov/llama.cpp\n","%cd llama.cpp\n","!make"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TRFww87h0ee0","outputId":"6519fd05-9b84-4e89-c11b-beee17a06f67","executionInfo":{"status":"ok","timestamp":1718777207474,"user_tz":-330,"elapsed":238159,"user":{"displayName":"NANDAKISHOR M","userId":"08970568302496220184"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'llama.cpp'...\n","remote: Enumerating objects: 27417, done.\u001b[K\n","remote: Counting objects: 100% (8952/8952), done.\u001b[K\n","remote: Compressing objects: 100% (567/567), done.\u001b[K\n","remote: Total 27417 (delta 8654), reused 8447 (delta 8384), pack-reused 18465\u001b[K\n","Receiving objects: 100% (27417/27417), 50.12 MiB | 22.72 MiB/s, done.\n","Resolving deltas: 100% (19625/19625), done.\n","/content/llama.cpp\n","I ccache not found. Consider installing it for faster compilation.\n","I llama.cpp build info: \n","I UNAME_S:   Linux\n","I UNAME_P:   x86_64\n","I UNAME_M:   x86_64\n","I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion \n","I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE \n","I NVCCFLAGS: -std=c++11 -O3 \n","I LDFLAGS:    \n","I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n","I CXX:       c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n","\n","cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion    -c ggml.c -o ggml.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c llama.cpp -o llama.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/common.cpp -o common.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/sampling.cpp -o sampling.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/grammar-parser.cpp -o grammar-parser.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/build-info.cpp -o build-info.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/json-schema-to-grammar.cpp -o json-schema-to-grammar.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c sgemm.cpp -o sgemm.o\n","cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n","cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n","cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c unicode.cpp -o unicode.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c unicode-data.cpp -o unicode-data.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/train.cpp -o train.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/baby-llama/baby-llama.cpp -o examples/baby-llama/baby-llama.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/baby-llama/baby-llama.o -o llama-baby-llama  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/batched/batched.cpp -o examples/batched/batched.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/batched/batched.o -o llama-batched  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/batched-bench/batched-bench.cpp -o examples/batched-bench/batched-bench.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  build-info.o ggml.o llama.o common.o sampling.o grammar-parser.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/batched-bench/batched-bench.o -o llama-batched-bench  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/llama-bench/llama-bench.cpp -o examples/llama-bench/llama-bench.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/llama-bench/llama-bench.o -o llama-bench  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/benchmark/benchmark-matmult.cpp -o examples/benchmark/benchmark-matmult.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  build-info.o ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/benchmark/benchmark-matmult.o -o llama-benchmark-matmult  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/console.cpp -o console.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/main/main.cpp -o examples/main/main.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o console.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/main/main.o -o llama-cli  \n","\n","====  Run ./llama-cli -h for help.  ====\n","\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp -o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o -o llama-convert-llama2c-to-ggml  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/embedding/embedding.cpp -o examples/embedding/embedding.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/embedding/embedding.o -o llama-embedding  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/eval-callback/eval-callback.cpp -o examples/eval-callback/eval-callback.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/eval-callback/eval-callback.o -o llama-eval-callback  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/export-lora/export-lora.cpp -o examples/export-lora/export-lora.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/export-lora/export-lora.o -o llama-export-lora  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/finetune/finetune.cpp -o examples/finetune/finetune.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/finetune/finetune.o -o llama-finetune  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/gbnf-validator/gbnf-validator.cpp -o examples/gbnf-validator/gbnf-validator.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gbnf-validator/gbnf-validator.o -o llama-gbnf-validator  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/gguf/gguf.cpp -o examples/gguf/gguf.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gguf/gguf.o -o llama-gguf  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/gguf-split/gguf-split.cpp -o examples/gguf-split/gguf-split.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gguf-split/gguf-split.o -o llama-gguf-split  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/gritlm/gritlm.cpp -o examples/gritlm/gritlm.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gritlm/gritlm.o -o llama-gritlm  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/imatrix/imatrix.cpp -o examples/imatrix/imatrix.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/imatrix/imatrix.o -o llama-imatrix  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/infill/infill.cpp -o examples/infill/infill.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o console.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/infill/infill.o -o llama-infill  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/llava/llava-cli.cpp -o examples/llava/llava-cli.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/llava/clip.cpp  -o examples/llava/clip.o -Wno-cast-qual\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/llava/llava.cpp -o examples/llava/llava.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/llava/llava-cli.o examples/llava/clip.o examples/llava/llava.o -o llama-llava-cli  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/lookahead/lookahead.cpp -o examples/lookahead/lookahead.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookahead/lookahead.o -o llama-lookahead  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/ngram-cache.cpp -o ngram-cache.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup.cpp -o examples/lookup/lookup.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup.o -o llama-lookup  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-create.cpp -o examples/lookup/lookup-create.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-create.o -o llama-lookup-create  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-merge.cpp -o examples/lookup/lookup-merge.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-merge.o -o llama-lookup-merge  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-stats.cpp -o examples/lookup/lookup-stats.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-stats.o -o llama-lookup-stats  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/parallel/parallel.cpp -o examples/parallel/parallel.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/parallel/parallel.o -o llama-parallel  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/passkey/passkey.cpp -o examples/passkey/passkey.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/passkey/passkey.o -o llama-passkey  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/perplexity/perplexity.cpp -o examples/perplexity/perplexity.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/perplexity/perplexity.o -o llama-perplexity  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c pocs/vdot/q8dot.cpp -o pocs/vdot/q8dot.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o pocs/vdot/q8dot.o -o llama-q8dot  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/quantize/quantize.cpp -o examples/quantize/quantize.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/quantize/quantize.o -o llama-quantize  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/quantize-stats/quantize-stats.cpp -o examples/quantize-stats/quantize-stats.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  build-info.o ggml.o llama.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/quantize-stats/quantize-stats.o -o llama-quantize-stats  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/retrieval/retrieval.cpp -o examples/retrieval/retrieval.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/retrieval/retrieval.o -o llama-retrieval  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/save-load-state/save-load-state.cpp -o examples/save-load-state/save-load-state.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/save-load-state/save-load-state.o -o llama-save-load-state  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/server/server.cpp -o examples/server/server.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o -Iexamples/server examples/server/server.o -o llama-server   \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/simple/simple.cpp -o examples/simple/simple.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/simple/simple.o -o llama-simple  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/speculative/speculative.cpp -o examples/speculative/speculative.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/speculative/speculative.o -o llama-speculative  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/tokenize/tokenize.cpp -o examples/tokenize/tokenize.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/tokenize/tokenize.o -o llama-tokenize  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/train-text-from-scratch/train-text-from-scratch.cpp -o examples/train-text-from-scratch/train-text-from-scratch.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/train-text-from-scratch/train-text-from-scratch.o -o llama-train-text-from-scratch  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c pocs/vdot/vdot.cpp -o pocs/vdot/vdot.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o pocs/vdot/vdot.o -o llama-vdot  \n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/cvector-generator/cvector-generator.cpp -o examples/cvector-generator/cvector-generator.o\n","c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/cvector-generator/cvector-generator.o -o llama-cvector-generator  \n","cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n"]}]},{"cell_type":"code","source":["%cd /content/llama.cpp\n","!python3 convert-hf-to-gguf.py /content/final_weights_new --outtype f16"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l9Cf37JT19VC","outputId":"5a110920-405e-4aa3-c144-1ee73262dccb","executionInfo":{"status":"ok","timestamp":1718777905432,"user_tz":-330,"elapsed":65095,"user":{"displayName":"NANDAKISHOR M","userId":"08970568302496220184"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/llama.cpp\n","INFO:hf-to-gguf:Loading model: final_weights_new\n","INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n","INFO:hf-to-gguf:Set model parameters\n","INFO:hf-to-gguf:gguf: context length = 2048\n","INFO:hf-to-gguf:gguf: embedding length = 2048\n","INFO:hf-to-gguf:gguf: feed forward length = 5632\n","INFO:hf-to-gguf:gguf: head count = 32\n","INFO:hf-to-gguf:gguf: key-value head count = 4\n","INFO:hf-to-gguf:gguf: rope theta = 10000.0\n","INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n","INFO:hf-to-gguf:gguf: file type = 1\n","INFO:hf-to-gguf:Set model tokenizer\n","INFO:gguf.vocab:Setting special token type bos to 1\n","INFO:gguf.vocab:Setting special token type eos to 2\n","INFO:gguf.vocab:Setting special token type unk to 0\n","INFO:gguf.vocab:Setting special token type pad to 2\n","INFO:gguf.vocab:Setting add_bos_token to True\n","INFO:gguf.vocab:Setting add_eos_token to False\n","INFO:gguf.vocab:Setting chat_template to {% for message in messages %}\n","{% if message['role'] == 'user' %}\n","{{ '<|user|>\n","' + message['content'] + eos_token }}\n","{% elif message['role'] == 'system' %}\n","{{ '<|system|>\n","' + message['content'] + eos_token }}\n","{% elif message['role'] == 'assistant' %}\n","{{ '<|assistant|>\n","'  + message['content'] + eos_token }}\n","{% endif %}\n","{% if loop.last and add_generation_prompt %}\n","{{ '<|assistant|>' }}\n","{% endif %}\n","{% endfor %}\n","INFO:hf-to-gguf:Exporting model to '/content/final_weights_new/ggml-model-f16.gguf'\n","INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n","INFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {2048, 32000}\n","INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {2048, 32000}\n","INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {5632, 2048}\n","INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 5632}\n","INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {2048, 256}\n","INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {2048}\n","Writing: 100% 2.20G/2.20G [00:57<00:00, 38.3Mbyte/s]\n","INFO:hf-to-gguf:Model successfully exported to '/content/final_weights_new/ggml-model-f16.gguf'\n"]}]},{"cell_type":"code","source":["!./llama-quantize /content/final_weights_new/ggml-model-f16.gguf /content/final_weights_new/ggml-model-q4_k_m.gguf q4_k_m\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5xh6yr_X2JZo","outputId":"c999ca9e-5d0b-4ff9-f07a-ad4fa322beab","executionInfo":{"status":"ok","timestamp":1718778581693,"user_tz":-330,"elapsed":126981,"user":{"displayName":"NANDAKISHOR M","userId":"08970568302496220184"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["main: build = 3182 (623494a4)\n","main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n","main: quantizing '/content/final_weights_new/ggml-model-f16.gguf' to '/content/final_weights_new/ggml-model-q4_k_m.gguf' as Q4_K_M\n","llama_model_loader: loaded meta data with 26 key-value pairs and 201 tensors from /content/final_weights_new/ggml-model-f16.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = final_weights_new\n","llama_model_loader: - kv   2:                          llama.block_count u32              = 22\n","llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n","llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n","llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 4\n","llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 10000.000000\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                          general.file_type u32              = 1\n","llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32000\n","llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 64\n","llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n","llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 2\n","llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n","llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n","llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n","llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   45 tensors\n","llama_model_loader: - type  f16:  156 tensors\n","[   1/ 201]                        output.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q6_K .. size =   125.00 MiB ->    51.27 MiB\n","[   2/ 201]                    token_embd.weight - [ 2048, 32000,     1,     1], type =    f16, converting to q4_K .. size =   125.00 MiB ->    35.16 MiB\n","[   3/ 201]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[   4/ 201]                blk.0.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n","[   5/ 201]                blk.0.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[   6/ 201]                  blk.0.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[   7/ 201]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[   8/ 201]                  blk.0.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[   9/ 201]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  10/ 201]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  11/ 201]                  blk.0.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n","[  12/ 201]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  13/ 201]                blk.1.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n","[  14/ 201]                blk.1.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  15/ 201]                  blk.1.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  16/ 201]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  17/ 201]                  blk.1.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[  18/ 201]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  19/ 201]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  20/ 201]                  blk.1.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n","[  21/ 201]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  22/ 201]               blk.10.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  23/ 201]               blk.10.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  24/ 201]                 blk.10.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  25/ 201]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  26/ 201]                 blk.10.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[  27/ 201]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  28/ 201]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  29/ 201]                 blk.10.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[  30/ 201]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  31/ 201]               blk.11.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  32/ 201]               blk.11.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  33/ 201]                 blk.11.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  34/ 201]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  35/ 201]                 blk.11.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[  36/ 201]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  37/ 201]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  38/ 201]                 blk.11.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[  39/ 201]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  40/ 201]               blk.12.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n","[  41/ 201]               blk.12.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  42/ 201]                 blk.12.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  43/ 201]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  44/ 201]                 blk.12.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[  45/ 201]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  46/ 201]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  47/ 201]                 blk.12.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n","[  48/ 201]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  49/ 201]               blk.13.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  50/ 201]               blk.13.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  51/ 201]                 blk.13.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  52/ 201]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  53/ 201]                 blk.13.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[  54/ 201]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  55/ 201]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  56/ 201]                 blk.13.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[  57/ 201]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  58/ 201]               blk.14.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  59/ 201]               blk.14.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  60/ 201]                 blk.14.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  61/ 201]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  62/ 201]                 blk.14.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[  63/ 201]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  64/ 201]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  65/ 201]                 blk.14.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[  66/ 201]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  67/ 201]               blk.15.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n","[  68/ 201]               blk.15.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  69/ 201]                 blk.15.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  70/ 201]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  71/ 201]                 blk.15.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[  72/ 201]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  73/ 201]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  74/ 201]                 blk.15.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n","[  75/ 201]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  76/ 201]               blk.16.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  77/ 201]               blk.16.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  78/ 201]                 blk.16.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  79/ 201]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  80/ 201]                 blk.16.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[  81/ 201]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  82/ 201]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  83/ 201]                 blk.16.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[  84/ 201]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  85/ 201]               blk.17.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  86/ 201]               blk.17.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  87/ 201]                 blk.17.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  88/ 201]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  89/ 201]                 blk.17.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[  90/ 201]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  91/ 201]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  92/ 201]                 blk.17.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[  93/ 201]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  94/ 201]               blk.18.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n","[  95/ 201]               blk.18.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  96/ 201]                 blk.18.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[  97/ 201]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  98/ 201]                 blk.18.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[  99/ 201]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 100/ 201]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 101/ 201]                 blk.18.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n","[ 102/ 201]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 103/ 201]               blk.19.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 104/ 201]               blk.19.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 105/ 201]                 blk.19.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 106/ 201]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 107/ 201]                 blk.19.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[ 108/ 201]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 109/ 201]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 110/ 201]                 blk.19.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[ 111/ 201]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 112/ 201]                blk.2.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 113/ 201]                blk.2.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 114/ 201]                  blk.2.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 115/ 201]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 116/ 201]                  blk.2.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[ 117/ 201]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 118/ 201]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 119/ 201]                  blk.2.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[ 120/ 201]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 121/ 201]               blk.20.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n","[ 122/ 201]               blk.20.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 123/ 201]                 blk.20.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 124/ 201]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 125/ 201]                 blk.20.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[ 126/ 201]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 127/ 201]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 128/ 201]                 blk.20.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n","[ 129/ 201]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 130/ 201]               blk.21.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 131/ 201]               blk.21.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 132/ 201]                 blk.21.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 133/ 201]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 134/ 201]                 blk.21.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[ 135/ 201]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 136/ 201]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 137/ 201]                 blk.21.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[ 138/ 201]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 139/ 201]                blk.3.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 140/ 201]                blk.3.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 141/ 201]                  blk.3.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 142/ 201]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 143/ 201]                  blk.3.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[ 144/ 201]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 145/ 201]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 146/ 201]                  blk.3.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[ 147/ 201]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 148/ 201]                blk.4.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n","[ 149/ 201]                blk.4.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 150/ 201]                  blk.4.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 151/ 201]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 152/ 201]                  blk.4.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[ 153/ 201]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 154/ 201]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 155/ 201]                  blk.4.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n","[ 156/ 201]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 157/ 201]                blk.5.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 158/ 201]                blk.5.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 159/ 201]                  blk.5.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 160/ 201]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 161/ 201]                  blk.5.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[ 162/ 201]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 163/ 201]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 164/ 201]                  blk.5.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[ 165/ 201]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 166/ 201]                blk.6.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 167/ 201]                blk.6.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 168/ 201]                  blk.6.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 169/ 201]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 170/ 201]                  blk.6.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[ 171/ 201]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 172/ 201]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 173/ 201]                  blk.6.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[ 174/ 201]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 175/ 201]                blk.7.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n","[ 176/ 201]                blk.7.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 177/ 201]                  blk.7.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 178/ 201]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 179/ 201]                  blk.7.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[ 180/ 201]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 181/ 201]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 182/ 201]                  blk.7.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n","[ 183/ 201]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 184/ 201]                blk.8.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n","[ 185/ 201]                blk.8.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 186/ 201]                  blk.8.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 187/ 201]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 188/ 201]                  blk.8.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[ 189/ 201]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 190/ 201]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 191/ 201]                  blk.8.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n","[ 192/ 201]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 193/ 201]                blk.9.ffn_down.weight - [ 5632,  2048,     1,     1], type =    f16, converting to q6_K .. size =    22.00 MiB ->     9.02 MiB\n","[ 194/ 201]                blk.9.ffn_gate.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 195/ 201]                  blk.9.ffn_up.weight - [ 2048,  5632,     1,     1], type =    f16, converting to q4_K .. size =    22.00 MiB ->     6.19 MiB\n","[ 196/ 201]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 197/ 201]                  blk.9.attn_k.weight - [ 2048,   256,     1,     1], type =    f16, converting to q4_K .. size =     1.00 MiB ->     0.28 MiB\n","[ 198/ 201]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 199/ 201]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 200/ 201]                  blk.9.attn_v.weight - [ 2048,   256,     1,     1], type =    f16, converting to q6_K .. size =     1.00 MiB ->     0.41 MiB\n","[ 201/ 201]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","llama_model_quantize_internal: model size  =  2098.35 MB\n","llama_model_quantize_internal: quant size  =   636.18 MB\n","\n","main: quantize time = 126317.48 ms\n","main:    total time = 126317.48 ms\n"]}]},{"cell_type":"code","source":["\n","from llama_cpp import Llama\n","llm = Llama(model_path=\"/content/final_weights_new/ggml-model-q4_k_m.gguf\",n_gpu_layers=30)\n","prompt = input_prompt.format(\n","        \"what is Nadi\", # input\n","        \"\"              # leave blank as response generated by AI\n","\n","    )\n","\n","output = llm(prompt, max_tokens=200)\n","out = output['choices'][0]['text']\n","generated_text = out\n","first_response = generated_text.split('### Input:')[0].strip()\n","\n","print(first_response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPS5lwYy2Sb7","outputId":"95dfbd23-b518-4aa6-f5af-8cef18c0af1e","executionInfo":{"status":"ok","timestamp":1717240425827,"user_tz":-330,"elapsed":2720,"user":{"displayName":"archana m","userId":"16614560235596268832"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_model_loader: loaded meta data with 26 key-value pairs and 201 tensors from /content/final_weights_new/ggml-model-q4_k_m.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = final_weights_new\n","llama_model_loader: - kv   2:                          llama.block_count u32              = 22\n","llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n","llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n","llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 4\n","llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 10000.000000\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                          general.file_type u32              = 15\n","llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32000\n","llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 64\n","llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n","llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 2\n","llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n","llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n","llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n","llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   45 tensors\n","llama_model_loader: - type q4_K:  135 tensors\n","llama_model_loader: - type q6_K:   21 tensors\n","llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 2048\n","llm_load_print_meta: n_embd           = 2048\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 4\n","llm_load_print_meta: n_layer          = 22\n","llm_load_print_meta: n_rot            = 64\n","llm_load_print_meta: n_embd_head_k    = 64\n","llm_load_print_meta: n_embd_head_v    = 64\n","llm_load_print_meta: n_gqa            = 8\n","llm_load_print_meta: n_embd_k_gqa     = 256\n","llm_load_print_meta: n_embd_v_gqa     = 256\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 5632\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 2048\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 1B\n","llm_load_print_meta: model ftype      = Q4_K - Medium\n","llm_load_print_meta: model params     = 1.10 B\n","llm_load_print_meta: model size       = 636.18 MiB (4.85 BPW) \n","llm_load_print_meta: general.name     = final_weights_new\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: PAD token        = 2 '</s>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size =    0.20 MiB\n","llm_load_tensors: offloading 22 repeating layers to GPU\n","llm_load_tensors: offloading non-repeating layers to GPU\n","llm_load_tensors: offloaded 23/23 layers to GPU\n","llm_load_tensors:        CPU buffer size =    35.16 MiB\n","llm_load_tensors:      CUDA0 buffer size =   601.02 MiB\n","....................................................................................\n","llama_new_context_with_model: n_ctx      = 512\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:      CUDA0 KV buffer size =    11.00 MiB\n","llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n","llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n","llama_new_context_with_model:      CUDA0 compute buffer size =    66.50 MiB\n","llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB\n","llama_new_context_with_model: graph nodes  = 710\n","llama_new_context_with_model: graph splits = 2\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n","Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.pre': 'default', 'llama.context_length': '2048', 'general.name': 'final_weights_new', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '5632', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '22', 'llama.attention.head_count_kv': '4', 'general.file_type': '15', 'llama.vocab_size': '32000', 'llama.rope.dimension_count': '64'}\n","Available chat formats from metadata: chat_template.default\n","Using gguf chat template: {% for message in messages %}\n","{% if message['role'] == 'user' %}\n","{{ '<|user|>\n","' + message['content'] + eos_token }}\n","{% elif message['role'] == 'system' %}\n","{{ '<|system|>\n","' + message['content'] + eos_token }}\n","{% elif message['role'] == 'assistant' %}\n","{{ '<|assistant|>\n","'  + message['content'] + eos_token }}\n","{% endif %}\n","{% if loop.last and add_generation_prompt %}\n","{{ '<|assistant|>' }}\n","{% endif %}\n","{% endfor %}\n","Using chat eos_token: </s>\n","Using chat bos_token: <s>\n","\n","llama_print_timings:        load time =      48.97 ms\n","llama_print_timings:      sample time =     127.92 ms /   128 runs   (    1.00 ms per token,  1000.63 tokens per second)\n","llama_print_timings: prompt eval time =      48.83 ms /    33 tokens (    1.48 ms per token,   675.84 tokens per second)\n","llama_print_timings:        eval time =    1071.43 ms /   127 runs   (    8.44 ms per token,   118.53 tokens per second)\n","llama_print_timings:       total time =    1390.02 ms /   160 tokens\n"]},{"output_type":"stream","name":"stdout","text":["Nadi is an offline Large Language Model (LLM) based AI PC application designed for AI education, eliminating the need for a real-time internet connection. It features a user-friendly interface and a variety of educational resources, including lecture notes, videos, diagrams, and Google Colab notebooks, making it a valuable tool for grasping coding basics and understanding fundamental concepts in machine learning and AI. Nadi's unique strength lies in its offline functionality, making it ideal for users with limited internet access or those who prefer a privacy-focused learning experience.\n"]}]}]}